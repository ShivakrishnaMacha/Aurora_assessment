# ğŸ§  Aurora QA API

A **question-answering microservice** built with **FastAPI** and **OpenRouter** that can infer answers to natural-language questions based on member messages fetched from a public dataset.

ğŸŒ **Live Demo:** [https://aurora-assessment.onrender.com](https://aurora-assessment.onrender.com)  
ğŸ“˜ **API Docs:** [https://aurora-assessment.onrender.com/docs](https://aurora-assessment.onrender.com/docs)


For example:
> - â€œWhen is Layla planning her trip to London?â€  
> - â€œHow many cars does Vikram Desai have?â€  
> - â€œWhat are Amiraâ€™s favorite restaurants?â€

---

## ğŸš€ Overview

This service fetches member messages from a provided public API and uses a **Large Language Model (LLM)** (via OpenRouterâ€™s API) to infer answers from those messages.  

It exposes a single endpoint `/ask` that accepts a question and responds with a concise, factual answer based only on the messages.

---

## ğŸ§© Example Usage

### Endpoint
GET /ask?question=When%20is%20Layla%20planning%20her%20trip%20to%20London%3F


### Example Response
```json
{
  "answer": "Layla is planning her trip to London in December."
}
If the information is not available:
{
  "answer": "I don't know."
}
```

## Setup Instructions

1ï¸âƒ£ Clone the repository
```
git clone https://github.com/<your-username>/aurora-qa-api.git
cd aurora-qa-api
```
2ï¸âƒ£ Create and activate a virtual environment
```
python3 -m venv venv
source venv/bin/activate
```
3ï¸âƒ£ Install dependencies
```
pip install -r requirements.txt
```
4ï¸âƒ£ Create .env file
```
OPENROUTER_API_KEY=your_api_key_here
```
5ï¸âƒ£ Run the service locally
```
uvicorn main:app --reload
```
The API will be available at:
ğŸ‘‰ http://127.0.0.1:8000

Deployed version (Render):
ğŸ‘‰ https://aurora-assessment.onrender.com


##ğŸ§  How It Works
1ï¸âƒ£ Fetching Data

The app retrieves member messages from:
```
https://november7-730026606190.europe-west1.run.app/messages
```
It paginates through results and caches them locally in messages_cache.json for faster subsequent queries.

2ï¸âƒ£ Building Context

All messages are combined into a contextual text block:
```
<user_name>: <message>
<user_name>: <message>
```

3ï¸âƒ£ Question Processing

When a question is received at /ask, the system sends both:

- the question, and
- the concatenated message context
to OpenRouterâ€™s LLM (Llama 3.3-70B-Instruct) model.

4ï¸âƒ£ Response Filtering

The LLM is prompted to:

- Only use information explicitly present in the messages
- Return â€œI donâ€™t know.â€ if uncertain


## ğŸ” API Documentation

| Endpoint | Method | Description |
|-----------|--------|-------------|
| `/` | GET | Health check |
| `/ask` | GET | Ask a question about the member messages |

### Parameters

| Name | Type | Required | Description |
|------|------|-----------|-------------|
| `question` | string | âœ… | Natural language question to be answered |

---

## ğŸ§ª Example Queries

| Question | Example Answer |
|-----------|----------------|
| When is Layla planning her trip to London? | â€œLayla is planning her trip to London in December.â€ |
| How many cars does Vikram Desai have? | â€œVikram Desai has 2 cars.â€ |
| What are Amiraâ€™s favorite restaurants? | â€œAmiraâ€™s favorite restaurants are Nobu and Dishoom.â€ |
| Who is going to Japan next month? | â€œI don't know.â€ |

## ğŸ§© Bonus 1: Design Notes

### ğŸ—ï¸ Approach 1 â€” LLM-based Retrieval (âœ… Chosen)

- Fetch all messages and use an LLM (via OpenRouter) to interpret the answer.  
- Simple to implement, flexible, and leverages model reasoning.  
- **Drawback:** Context window limits when message history grows very large.  

---

### ğŸ§® Approach 2 â€” Embedding Search + LLM

- Pre-compute embeddings for all messages (e.g., using OpenAI embeddings).  
- On query, find top-N relevant messages and send only those to the model.  
- More scalable but more complex to implement.  

---

### ğŸ“Š Approach 3 â€” Rule-based Extraction

- Use regex / keyword matching to infer structured facts.  
- Efficient but brittle and less adaptable to natural language.  

---

### âœ… Final Decision

> **Approach 1** â€” chosen because the dataset is small enough and prioritizing correctness & simplicity fits the assessment goals.

---

## ğŸ” Bonus 2: Data Insights

After analyzing the dataset returned by the `/messages` API:

- Some users have incomplete or inconsistent message patterns (e.g., missing user names or truncated messages).  
- Certain members appear multiple times under slightly different spellings (e.g., â€œLayla M.â€ vs â€œLayla Malikâ€).  
- Occasional duplicate or contradictory entries (e.g., two different trip dates for the same person).  
- Some messages include non-standard formatting (extra emojis, multiple question marks, etc.), which can affect LLM parsing.  

ğŸ§© These inconsistencies suggest the data likely originated from conversational logs, not structured forms.

---

## ğŸ§° Tech Stack

| Component | Description |
|------------|-------------|
| **Language** | Python 3.10+ |
| **Framework** | FastAPI |
| **LLM Provider** | OpenRouter (Llama 3.3-70B-Instruct) |
| **Env Management** | python-dotenv |
| **HTTP Requests** | requests |
| **Server** | uvicorn |

---

## ğŸ“¦ Project Structure

```text
aurora-qa-api/
â”‚
â”œâ”€â”€ main.py                # FastAPI app entry point
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ .env                   # Environment variables (excluded from git)
â”œâ”€â”€ .gitignore             # Ignore rules
â”œâ”€â”€ messages_cache.json    # Cached dataset (excluded)
â””â”€â”€ README.md              # Documentation
```


---

## ğŸ§ª Testing Locally

Run the development server:

```bash
uvicorn main:app --reload
```
Then open:

API Docs: http://127.0.0.1:8000/docs

Example Query:
http://127.0.0.1:8000/ask?question=When%20is%20Layla%20planning%20her%20trip%20to%20London%3F



